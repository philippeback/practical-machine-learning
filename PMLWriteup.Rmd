---
title: "Practical Machine Learning Assignment Writeup"
author: "Philippe Back"
date: "June 21, 2015"
output: html_document
---

#Background

In an experiment conducted and documented at [Human Activity Recognition Public Datasets](http://groupware.les.inf.puc-rio.br/har), 6 male participants, aged 20-28, lifted weights under supervision.

The point was to gather data on "how well" they lifted weights.

The results (Class) and the associated parameters were recorded.

Class | Meaning
------| -------
Class A | Specified execution of the exercise
Class B | Throwing elbows to the front
Class C | Lifting halfway
Class D | Lowering halfway
Class E | Throwing hips to front

The data set is about performing 10 reps of unilateral dumbell biceps curls.

# Exploring Data

## Getting Data
There are two datasets:

- (https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv), which is the data to use in order to establish and validate the predictive model.
- (https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv), which is the data to use for prediction based on the model. It also forms the data for which prediction has to be provided for an autograding part of the project.

```{r fetch}
# url<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
# dataLocation <- download.file(url, "./data/pml-training.csv", method="curl")

train_file <- './data/pml-training.csv'

# url<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
# dataLocation <- download.file(url, "./data/pml-testing.csv", method="curl")
test_file <- './data/pml-testing.csv'
```

```{r codenas}

set.seed(22121969)

trainRaw <- read.csv('./data/pml-training.csv', header=TRUE, ,na.strings=c('NA','#DIV/0!',' ',''))

testRaw <- read.csv('./data/pml-testing.csv', header=TRUE, ,na.strings=c('NA','#DIV/0!',' ',''))

# summary(trainRaw)

all_names <- names(trainRaw)
print(all_names)

```

## Cleaning Data

In order to predict, we have to keep predictors that make sense to use.

Looking at the file, there are columns that aren't suited as predictors.

Either a lot of NA's, or #DIV/0!, or empty or fields that aren't measurements. Those are in columns named after either:

- timestamps (raw_timestamp_part1, raw_timestamp_part2,ctvd_timestamp)
- timewindows 
- statistical aggregates (avg, kurtosis, skewness, var)
- the subject

We need to remove the unnecessary columns from the data.

We can keep the **classe** column as it is the outcome.

The statistical aggregates are only valid for the rows where **new_window** is *yes*. It looks as if those are summaries of the rows where **new_window** is *no*.

```{r onlyobs}
trainObservations <- trainRaw[trainRaw$new_window == 'no', ]
```

Remove all flavors of NAs by columns

```{r removenas}
rowsCount<-nrow(trainObservations)

sumsOfNAs <- apply(is.na(trainObservations), 2, sum) 
isFullOfNAs <- sumsOfNAs == rowsCount
hasData <- ! isFullOfNAs

trainClean <- trainObservations[,hasData]
```

Remove the first 7 columns which are not pertinent for the model as explained above.

```{r removecols}
print(names(trainClean)[1:7])
trainCleaner <- trainClean[,-c(1:7 )]
trainingSet <- trainCleaner
```

One can run 
```{r sumclean}
# summary(trainCleaner)
```
to see that the data is now clean.

# Building a prediction model

## Preparing the data

We'll take 10% for training the model. That allows to check if the model is any good by looking at its accuracy. 

```{r partition10pct}
library(caret)

trainIndex<-createDataPartition(trainingSet$classe, p=0.1, list=FALSE)

trainingPartition <- trainingSet[trainIndex,]
testingPartition <- trainingSet[-trainIndex,]


```

## Training

Let's train a model using random forests (this is deemed as one of the best algorithms, let's use that). With the 10% partition, random forests will not take forever to compute.

Note: there should be a trainControl() tuning but after having read about it, I'll stick to the defaults as I think I need more time to grasp it all.
The result is good enough anyway.
```{r loadpar}
#library(doParallel)
library(doMC)
library(foreach)
registerDoMC(cores=8)
```

```{r train10pct parallel-do}
model <- train(classe ~ ., 
               trainingPartition, 
               method="rf",
               trControl = trainControl(),
               tuneGrid = NULL, 
               tuneLength = 3
)
```

Let's see the importance of the variables:

```{r plotimp}
plot(varImp(model))
```

Let's now check if this model is any good.

```{r confmtx}
prediction <- predict(model, newdata=trainingPartition)
mtx<-confusionMatrix(prediction, trainingPartition$classe)
print(mtx)
print(paste("Accuracy: ", round(mtx$overall['Accuracy']*100,2),'%'))
```

Accuracy 100% on the training set.

On the testing part, we get:

```{r confmtxtest}
testPrediction <- predict(model, newdata=testingPartition)
mtxTest<-confusionMatrix(testPrediction, testingPartition$classe)
print(mtxTest)
print(paste("Accuracy: ", round(mtxTest$overall['Accuracy']*100,2),'%'))
```

Looks like it is decent enough with `r paste("Accuracy: ", round(mtxTest$overall['Accuracy']*100,2),'%')` using 10% of the data for training.

## Training on more data to get better accuracy

Let's try with 60% of the data

```{r part60pct}
trainIndex<-createDataPartition(trainingSet$classe, p=0.60, list=FALSE)

trainingPartition2<- trainingSet[trainIndex,]
testingPartition2 <- trainingSet[-trainIndex,]
```

```{r train60pct parallel-do}
model2 <- train(classe ~ ., 
               trainingPartition2, 
               method="rf",
               trControl = trainControl(),
               tuneGrid = NULL, 
               tuneLength = 3
)
```


On the testing part, we now get:

```{r testmtx}
testPrediction2 <- predict(model2, newdata=testingPartition2)
mtxTest2<-confusionMatrix(testPrediction2, testingPartition2$classe)
print(mtxTest2)
print(paste("Accuracy: ", round(mtxTest2$overall['Accuracy']*100,2),'%'))
```

## Out of sample error rate

This is 100%-Accuracy

```{r ooser}
print(paste("Out of sample error rate: ", round((1.0-mtxTest2$overall['Accuracy'])*100,2),'%'))
```


# Applying the model to the testing set

```{r finaltest}
predictionTest<-predict(model2, newdata=testRaw)
```

Let's write the files for the submission:

```{r writefiles}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

print("Answers:")
print(predictionTest)
pml_write_files(predictionTest)
```

All of the predictions appear to be correct on the prediction submissions.

# Conclusion

This has been an interesting assignment.

The fact that all tests are predicted correctly is probably due to the fact that the test entries do match the subjects that are part of the training set and that the experiment was well controlled.

Now, due to time constraints, I've been using all predictors but varImp(model) shows that these are not all equal. Maybe it could be as good with only 10 covariates.

Also, they may be correlations between the top variables. To be investigated further.

For the record, I've been running this on a quad core i7 4770K 3.85 Ghz with 16GB or RAM.
The OS is Ubuntu Linux.

The 60% training took around 10-15 minutes to complete.

I ran into some issues with the parallel computing and then used the doMC lib successfuly to use all cores easily as documented on the "parallel" webpage of caret documentation.





